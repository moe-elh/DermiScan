{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data._utils.collate import default_collate\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import os\n",
    "from PIL import Image\n",
    "import xml.etree.ElementTree as ET\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import tempfile\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import torchvision.models.detection as detection\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, FasterRCNN_ResNet50_FPN_Weights\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import json\n",
    "from torchvision.ops import nms\n",
    "import logging\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'dataset\\\\train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 202\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m#it is also a dictionary that calculates the class weight based on the inverse of the class count which is a dictionary that is defined earlier\u001b[39;00m\n\u001b[0;32m    200\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m {class_to_int[\u001b[38;5;28mcls\u001b[39m]: \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m/\u001b[39m count \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m, count \u001b[38;5;129;01min\u001b[39;00m class_counts\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m--> 202\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mAcneSet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdataset\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mset_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAcneSet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 97\u001b[0m, in \u001b[0;36mAcneSet.__init__\u001b[1;34m(self, root_dir, set_type, transform)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root_dir, set_type)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mannotation_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root_dir, set_type)\n\u001b[1;32m---> 97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_dir\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'dataset\\\\train'"
     ]
    }
   ],
   "source": [
    "#importing needed packages and modules \n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as F\n",
    "from PIL import Image\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import sys\n",
    "from torch.utils.data._utils.collate import default_collate\n",
    "\n",
    "\"\"\"\n",
    "collate function :\n",
    "this function will collate data samples into batches \n",
    "images and boxes which are the annotations are seperated from each batch \n",
    "we will be used default collate to collate the images which is a function provided by pytorch \n",
    "targets are returned as a list of dictionaries \n",
    "\"\"\"\n",
    "def collate_fn(batch):\n",
    "    batch_images = [item[0] for item in batch]\n",
    "    batch_targets = [item[1] for item in batch]\n",
    "    batch_images_collated = default_collate(batch_images)\n",
    "    return batch_images_collated, batch_targets\n",
    "\n",
    "#category_id_mapping is a dictionary that maps the IDs to different acne type \n",
    "category_id_mapping = {\n",
    "    1: 'Acne',\n",
    "    2: 'Blackhead',\n",
    "    3: 'Conglobata',\n",
    "    4: 'Crystanlline',\n",
    "    5: 'Cystic',\n",
    "    6: 'Flat_wart',\n",
    "    7: 'Folliculitis',\n",
    "    8: 'Keloid',\n",
    "    9: 'Milium',\n",
    "    10: 'Papular',\n",
    "    11: 'Purulent',\n",
    "    12: 'Scars',\n",
    "    13: 'Sebo-crystan-conglo',\n",
    "    14: 'Syringoma',\n",
    "    15: 'Whitehead',\n",
    "    16: 'null'\n",
    "}\n",
    "\n",
    "# this is the reverse of category_id_mapping as it provides category of the acne to its integer which represents the ID \n",
    "name_to_id = {v: k for k, v in category_id_mapping.items()}\n",
    "\n",
    "'''\n",
    "next we have created different classes that implement different transofrmations to the images while preserving the bounding boxes\n",
    "'''\n",
    "#ToTensorWithTarget this class will convert PIL images to pytorch tensor and the bounding boxes are not changes \n",
    "class ToTensorWithTarget:\n",
    "    def __call__(self, image, target):\n",
    "        return transforms.ToTensor()(image), target\n",
    "    \n",
    "#ColorJitterWithTarget will apply different color editing such as brightness, contrast, saturation and hue to the images while also not making any changes to the bounding bounding boxes \n",
    "class ColorJitterWithTarget:\n",
    "    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0):\n",
    "        self.color_jitter = transforms.ColorJitter(brightness=brightness, contrast=contrast, saturation=saturation, hue=hue)\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        image = self.color_jitter(image)\n",
    "        return image, target\n",
    "    \n",
    "#RandomHorizontalFlipWithBBox  will flip the images horizantally while also adjusting the bounding boxes accornding to the flipes that were made to the images \n",
    "class RandomHorizontalFlipWithBBox(transforms.RandomHorizontalFlip):\n",
    "    def __call__(self, image, target):\n",
    "        if torch.rand(1) < self.p:\n",
    "            image = F.hflip(image)\n",
    "            if 'boxes' in target and target['boxes'].numel() > 0:\n",
    "                width = image.shape[-1]\n",
    "                xmin = width - target['boxes'][:, 2]\n",
    "                xmax = width - target['boxes'][:, 0]\n",
    "                target['boxes'][:, 0] = xmin\n",
    "                target['boxes'][:, 2] = xmax\n",
    "        return image, target\n",
    "    \n",
    "#ComposeWithTarget different transformations while also having transformation to the bouding boxes \n",
    "class ComposeWithTarget:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "\n",
    "#AcneSet this is a class that contains the data set \n",
    "class AcneSet(Dataset):\n",
    "    # __init__ will initialzie the data set that are found in the directory a\n",
    "    def __init__(self, root_dir, set_type='train', transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.set_type = set_type\n",
    "        self.transform = transform\n",
    "        self.image_dir = os.path.join(root_dir, set_type)\n",
    "        self.annotation_dir = os.path.join(root_dir, set_type)\n",
    "        self.images = [f for f in os.listdir(self.image_dir) if f.endswith('.jpg')]\n",
    "\n",
    "    # will return the number of images that we have \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    # will load the image and its corrisponding bounding boxes \n",
    "    # it will extract the bouding boxes and the class labels \n",
    "    def __getitem__(self, idx):\n",
    "        # constructs the full path of the image including bounding file \n",
    "        img_filename = self.images[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_filename)\n",
    "        annotation_path = os.path.join(self.annotation_dir, img_filename.replace('.jpg', '.xml'))\n",
    "        # loading the data and convert it to RBG if it is not already in it using convert('RBG')\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        # pase the annotation boxes that are associated with the images using ET.parse to extract the bounding boxes \n",
    "        tree = ET.parse(annotation_path)\n",
    "        root = tree.getroot()\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        # iterate over the object tag that is found in the tree ( the bounding boxes) in order to extract them and gather data about them and the class labels \n",
    "        for member in root.findall('object'):\n",
    "            # retrive the name of the class \n",
    "            class_name = member.find('name').text \n",
    "            # map the class name to the integer ID using name_to_id dictrionay that was defined at the start of the code \n",
    "            class_id = name_to_id.get(class_name)  \n",
    "            if class_id is not None: \n",
    "                # if the class name exists append class id to the labels and extract the bounding boxes \n",
    "                labels.append(class_id)\n",
    "                # create a list of the bounding boxes for all the detected images \n",
    "                bbox = member.find('bndbox')\n",
    "                xmin = float(bbox.find('xmin').text)\n",
    "                ymin = float(bbox.find('ymin').text)\n",
    "                xmax = float(bbox.find('xmax').text)\n",
    "                ymax = float(bbox.find('ymax').text)\n",
    "                boxes.append([xmin, ymin, xmax, ymax])\n",
    "        #convert both lists the list of boxes and the list of labels to pytorch using torch.as_tensor\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 4), dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        # create a target dictionary that contains the boxes and the class labels and the image id \n",
    "        target = {'boxes': boxes, 'labels': labels, 'image_id': torch.tensor([idx])}\n",
    "        if self.transform:\n",
    "            image, target = self.transform(image, target)\n",
    "        return image, target\n",
    "\n",
    "\n",
    "    '''\n",
    "    get_transform is a static method thus it belongs to the class itself \n",
    "    it returns the ComposeWithTarget  which is a list that is composed of the data augmentation transformation that has been applied to the image and their annotation boxes \n",
    "    its arugment is train which is a boolean which indicates if those transformations are done for training if true and if it is false then it is for testing and validating \n",
    "    it \n",
    "    '''\n",
    "    @staticmethod\n",
    "    def get_transform(train=True):\n",
    "        transforms_list = [\n",
    "            ToTensorWithTarget(),\n",
    "            ColorJitterWithTarget(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "            RandomHorizontalFlipWithBBox(p=0.5)\n",
    "        ]\n",
    "        return ComposeWithTarget(transforms_list)\n",
    "\n",
    "#this is a dictionary that maps the class name  or the classification of the acne to the ID \n",
    "class_to_int = {\n",
    "    'Acne': 1,\n",
    "    'Blackhead': 2,\n",
    "    'Conglobata': 3,\n",
    "    'Crystanlline': 4,\n",
    "    'Cystic': 5,\n",
    "    'Flat_wart': 6,\n",
    "    'Folliculitis': 7,\n",
    "    'Keloid': 8,\n",
    "    'Milium': 9,\n",
    "    'Papular': 10,\n",
    "    'Purulent': 11,\n",
    "    'Scars': 12,\n",
    "    'Sebo-crystan-conglo': 13,\n",
    "    'Syringoma': 14,\n",
    "    'Whitehead': 15,\n",
    "    'null': 16\n",
    "}\n",
    "\n",
    "# this is a dictionary that contains the count of each class that is in our data set \n",
    "class_counts = {\n",
    "    'Acne': 5314,\n",
    "    'Blackhead': 799,\n",
    "    'Conglobata': 8,\n",
    "    'Crystanlline': 62,\n",
    "    'Cystic': 438,\n",
    "    'Flat_wart': 240,\n",
    "    'Folliculitis': 237,\n",
    "    'Keloid': 255,\n",
    "    'Milium': 330,\n",
    "    'Papular': 1457,\n",
    "    'Purulent': 1183,\n",
    "    'Scars': 388,\n",
    "    'Sebo-crystan-conglo': 548,\n",
    "    'Syringoma': 115,\n",
    "    'Whitehead': 775,\n",
    "    'null': 50\n",
    "}\n",
    "\n",
    "\n",
    "#it is also a dictionary that calculates the class weight based on the inverse of the class count which is a dictionary that is defined earlier\n",
    "class_weights = {class_to_int[cls]: 1.0 / count for cls, count in class_counts.items()}\n",
    "\n",
    "train_dataset = AcneSet(root_dir='dataset', set_type='train', transform=AcneSet.get_transform(train=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights exist already, loading them\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "this code will handle the weights for each sample in the data set for training \n",
    "'''\n",
    "\n",
    "# it will first check if samples are found in the folder\n",
    "weights_file = 'sample_weights.pt'\n",
    "# if the samples dont exist then it will calculate the weights for each sample in the data set for the training based on the class weights and it will also save them to a file \n",
    "if not os.path.exists(weights_file):\n",
    "    print(\"weights dont exist, generating\")\n",
    "    total_samples = len(train_dataset)\n",
    "    sample_weights = []\n",
    "    '''\n",
    "    for each weight it will calculate it based on the class weight with the label of each image \n",
    "    if the label doesnt have a weight in the class weight then it sets it 0 \n",
    "    '''\n",
    "    for idx in range(len(train_dataset)):\n",
    "        _, target = train_dataset[idx]\n",
    "        label_weights = [class_weights.get(label.item(), 0) for label in target['labels']]\n",
    "        weight = sum(label_weights) / len(label_weights) if label_weights else 0\n",
    "        sample_weights.append(weight)\n",
    "        sys.stdout.write(f\"\\rProcessed sample {idx+1}/{total_samples}\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    sample_weights = torch.tensor(sample_weights)\n",
    "    # it will save the weights that have been calculates to the file sample_weights.pt using the torch.save\n",
    "    torch.save(sample_weights, weights_file)\n",
    "    print(\"done saving weights to file\")\n",
    "else:\n",
    "    print(\"weights exist already, loading them\")\n",
    "    sample_weights = torch.load(weights_file)\n",
    "'''\n",
    "we created weighted sampler using the same weight it is used to sample fromt the dataset during the trianing it will also set replacement to true , \n",
    "the number of samples equals to the number of samples that is found in the data set \n",
    "'''\n",
    "weighted_sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef plot_metrics(train_losses, val_precisions, val_recalls, val_f1_scores, val_maps):\\n    epochs = range(1, len(train_losses) + 1) \\n    plt.figure(figsize=(20, 4))\\n\\n    # Plotting training loss\\n    plt.subplot(1, 5, 1)\\n    plt.plot(epochs, train_losses, 'b-o', label='Train Loss')\\n    plt.xlabel('Epochs')\\n    plt.ylabel('Loss')\\n    plt.title('Training Loss')\\n    plt.legend()\\n\\n    # Plotting precision\\n    plt.subplot(1, 5, 2)\\n    plt.plot(epochs, val_precisions, 'orange-o', label='Validation Precision')\\n    plt.xlabel('Epochs')\\n    plt.ylabel('Precision')\\n    plt.title('Validation Precision')\\n    plt.legend()\\n\\n    # Plotting recall\\n    plt.subplot(1, 5, 3)\\n    plt.plot(epochs, val_recalls, 'g-o', label='Validation Recall')\\n    plt.xlabel('Epochs')\\n    plt.ylabel('Recall')\\n    plt.title('Validation Recall')\\n    plt.legend()\\n\\n    # Plotting F1 Score\\n    plt.subplot(1, 5, 4)\\n    plt.plot(epochs, val_f1_scores, 'r-o', label='Validation F1 Score')\\n    plt.xlabel('Epochs')\\n    plt.ylabel('F1 Score')\\n    plt.title('Validation F1 Score')\\n    plt.legend()\\n\\n    # Plotting mAP\\n    plt.subplot(1, 5, 5)\\n    plt.plot(epochs, val_maps, 'purple-o', label='Validation mAP')\\n    plt.xlabel('Epochs')\\n    plt.ylabel('mAP')\\n    plt.title('Validation Mean Average Precision (mAP)')\\n    plt.legend()\\n\\n    plt.tight_layout()\\n    plt.show()\\n    \""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "'''\n",
    "train_epoch this function trains the model for each epoch \n",
    "using the data loader and optimizer \n",
    "it will iterate over the batches of the data \n",
    "it will move those batches to a specific device \n",
    "it will compute the loss \n",
    "it will performs backpropagation\n",
    "it will update the models parameter \n",
    "it returns the average training loss for every epoch it trained on \n",
    "'''\n",
    "def train_epoch(model, train_loader, optimizer, device, epoch, num_epochs):\n",
    "    # sets the model to training mode \n",
    "    model.train()\n",
    "    # initalizing variabales to keep track of the total loss and the total batches in the training laoder\n",
    "    total_loss = 0\n",
    "    total_batches = len(train_loader)\n",
    "    #it will iterate over each batch using enumerate(trainloder)\n",
    "    for batch_idx, (images, targets) in enumerate(train_loader):\n",
    "        # images and targets are moves to a specific device\n",
    "        images = [image.to(device) for image in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        # clear the gards of the optimizer \n",
    "        optimizer.zero_grad()\n",
    "        # compute the loss for each batch in the trainin g\n",
    "        loss_dict = model(images, targets)\n",
    "        # calculates the total loss by summing the individual loss using sum\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        losses.backward()\n",
    "        # update the model optimizer \n",
    "        optimizer.step()\n",
    "        total_loss += losses.item()\n",
    "        # printing the batches \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Batch {batch_idx+1}/{total_batches}, Loss: {losses.item():.4f}\")\n",
    "    avg_loss = total_loss / total_batches\n",
    "    print(f\"Average training loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "'''\n",
    "calculate the IoU whichis the intersection over the union betweent he predicted boxes and the ground to truth boxes \n",
    "input: boxes-preds and boxes-gt which are arrays each boxes represents the bounding boxes in the format of [x-min,y-min,x-max,x-min]\n",
    "'''\n",
    "def calculate_iou_vectorized(boxes_preds, boxes_gt):\n",
    "    # xA y A xB yB represent the corrdinates of the intersection between the two boxes\n",
    "    xA = np.maximum(boxes_preds[:, None, 0], boxes_gt[None, :, 0])\n",
    "    yA = np.maximum(boxes_preds[:, None, 1], boxes_gt[None, :, 1])\n",
    "    xB = np.minimum(boxes_preds[:, None, 2], boxes_gt[None, :, 2])\n",
    "    yB = np.minimum(boxes_preds[:, None, 3], boxes_gt[None, :, 3])\n",
    "    # calculate the area of the intersection between the two boxes\n",
    "    interArea = np.maximum(0, xB - xA + 1) * np.maximum(0, yB - yA + 1)\n",
    "    # calculate the area of each box alone\n",
    "    boxAArea = (boxes_preds[:, 2] - boxes_preds[:, 0] + 1) * (boxes_preds[:, 3] - boxes_preds[:, 1] + 1)\n",
    "    boxBArea = (boxes_gt[:, 2] - boxes_gt[:, 0] + 1) * (boxes_gt[:, 3] - boxes_gt[:, 1] + 1)\n",
    "    # calculate the IoU for each box and return it as a numpy array \n",
    "    iou = interArea / (boxAArea[:, None] + boxBArea - interArea)\n",
    "\n",
    "    return iou\n",
    "'''\n",
    "get_model_predictions will generate predicitons using a trained object detection model \n",
    "'''\n",
    "def get_model_predictions(model, data_loader, device):\n",
    "    # sets the model to evaluation mode\n",
    "    model.eval()\n",
    "    # initalizes the array to store the predicitons that will be generated\n",
    "    all_predictions = []\n",
    "    with torch.no_grad():\n",
    "        # it will iterate over the batches that are in the data loader \n",
    "        for images, targets in tqdm(data_loader, desc=\"Predicting\", leave=False):\n",
    "            # it will move the images to a specific device and compute the predicition using the model\n",
    "            images = list(img.to(device) for img in images)\n",
    "            outputs = model(images)\n",
    "            # the predicited  boxes and scores and labels are extracted and converted to a comment format \n",
    "            for output in outputs:\n",
    "                boxes = output['boxes'].data.cpu().numpy()\n",
    "                scores = output['scores'].data.cpu().numpy()\n",
    "                labels = output['labels'].data.cpu().numpy()\n",
    "                # the predicitions are stores in a form of dictionary \n",
    "                for box, score, label in zip(boxes, scores, labels):\n",
    "                    bbox = [box[0], box[1], box[2] - box[0], box[3] - box[1]]\n",
    "                    all_predictions.append({'bbox': bbox, 'score': score, 'label': label})\n",
    "    # a log message to indicate that the predicitions were successfully generated \n",
    "    logging.info(\"Predictions generated.\")\n",
    "    return all_predictions\n",
    "'''\n",
    "get_ground_truths the function will extract the ground truth and labels using the data loader \n",
    "'''\n",
    "def get_ground_truths(data_loader, category_id_mapping):\n",
    "    # initalize an array for the groun truth\n",
    "    ground_truths = []\n",
    "    # iterate through the batches of the data from the data loader\n",
    "    # extract the images, boxes, labels\n",
    "    for images, targets in tqdm(data_loader, desc=\"Extracting Ground Truths\", leave=False):\n",
    "        for target in targets:\n",
    "            # bounding boxes are converted to the format [x-min,x-max,y-min,y-max]\n",
    "            image_id = target[\"image_id\"].item()\n",
    "            boxes = target['boxes'].data.cpu().numpy()\n",
    "            labels = target['labels'].data.cpu().numpy()\n",
    "            for box, label in zip(boxes, labels):\n",
    "                # the information is then stored in the form of dictionary \n",
    "                bbox = [box[0], box[1], box[2] - box[0], box[3] - box[1]]\n",
    "                # the keys of the dictionary are image_id, catergory_id, bbox\n",
    "                ground_truths.append({\n",
    "                    'image_id': image_id,\n",
    "                    'category_id': category_id_mapping[label.item()],\n",
    "                    'bbox': bbox\n",
    "                })\n",
    "    # after processing all the batches a log message is printed indicating sucess  \n",
    "    logging.info(\"Ground truths extraction completed.\")\n",
    "    return ground_truths\n",
    "'''\n",
    "calculate_metrics_vectorized calculate the precision and the f1 score \n",
    "'''\n",
    "def calculate_metrics_vectorized(predictions, ground_truths, iou_threshold=0.5):\n",
    "    # extracting the boxes of predicition and the ground to truth \n",
    "    boxes_preds = np.array([p['bbox'] for p in predictions])\n",
    "    boxes_gt = np.array([gt['bbox'] for gt in ground_truths])\n",
    "    labels_gt = np.array([gt['category_id'] for gt in ground_truths])  \n",
    "    # calculate the IoU by calling the previously defined calculate_iou_vectorized function\n",
    "    iou_matrix = calculate_iou_vectorized(boxes_preds, boxes_gt)\n",
    "    # a logical matrix 'matches' is created \n",
    "    matches = (iou_matrix >= iou_threshold)\n",
    "    # true positive is calculated by summing the matches per ground truth label\n",
    "    tp = np.sum(matches.any(axis=0))\n",
    "    # false positive is calculates by summing the matches per predicition label where no matches with ground truth exists \n",
    "    fp = np.sum(~matches.any(axis=1))\n",
    "    # False negatives calculated by subtracting true positive from the total number of the ground truth \n",
    "    fn = len(ground_truths) - tp\n",
    "    # calculating precision and recall and the F1 score \n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return precision, recall, f1_score\n",
    "'''\n",
    "simplified_map computed the mean average precisions\n",
    "'''\n",
    "def simplified_map(predictions, ground_truths, iou_thresholds=[0.5]):\n",
    "    # initalizing an array to store the output\n",
    "    average_precisions = []\n",
    "    #iterating over each threshold \n",
    "    for iou_threshold in iou_thresholds:\n",
    "        # calculates the precision using a predefined function calculate_metrics_vectorized\n",
    "        precision, _, _ = calculate_metrics_vectorized(predictions, ground_truths, iou_threshold=iou_threshold)\n",
    "        # append the result to the average precision array \n",
    "        average_precisions.append(precision)\n",
    "    # compute the mean average and return it \n",
    "    return sum(average_precisions) / len(average_precisions)\n",
    "'''\n",
    "validate this function will perform validation using precision and recall and F1 score and the mAP\n",
    "'''\n",
    "def validate(model, data_loader, device, category_id_mapping):\n",
    "    print(\"Starting validation...\")\n",
    "    # sets the model to evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # it will generate a predicition and ground truth using predefined functions get_model_predictions get_ground_truths\n",
    "        predictions = get_model_predictions(model, data_loader, device)\n",
    "        ground_truths = get_ground_truths(data_loader, category_id_mapping)\n",
    "        # it will then calculate the precision and the recall and the f1 score using a predefined function calculate_metrics_vectorized\n",
    "        precision, recall, f1_score = calculate_metrics_vectorized(predictions, ground_truths, iou_threshold=0.5)\n",
    "        # it will then calculate mAP using a pre defined funciton simplified_map\n",
    "        mAP = simplified_map(predictions, ground_truths, iou_thresholds=[0.5])\n",
    "        print(f\"Validation completed. Precision: {precision * 100:.2f}%, Recall: {recall * 100:.2f}%, F1 Score: {f1_score * 100:.2f}%, mAP: {mAP:.2f}\")\n",
    "        return precision, recall, f1_score, mAP\n",
    "\n",
    "'''\n",
    "this function will plot the trianing loss, validation precisssion and the validation recall , f1 score and the validation mAP over each epoch\n",
    "'''\n",
    "def plot_metrics(train_losses, val_precisions, val_recalls, val_f1_scores, val_maps):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(20, 4))\n",
    "\n",
    "    # creating a subplot for every matix \n",
    "    plt.subplot(1, 5, 1)\n",
    "    plt.plot(epochs, train_losses, 'b-o', label='Train Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.legend()\n",
    "    # Function to filter out None values and multiply by 100\n",
    "    def prepare_for_plotting(metrics):\n",
    "        return [m * 100 for m in metrics if m is not None]\n",
    "\n",
    "'''\n",
    "This function plots the training loss over epochs in a separate plot.\n",
    "'''\n",
    "def plot_training_loss(train_losses):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(epochs, train_losses, 'b-o', label='Train Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "'''\n",
    "his function plots validation precision, recall, F1-score, and mAP over epochs in subplots. \n",
    "It supports plotting multiple metrics simultaneously and determines the layout of subplots based on the number of metrics to plot.\n",
    "'''\n",
    "def plot_metrics(val_precisions, val_recalls, val_f1_scores, val_maps):\n",
    "    valid_epochs = range(1, len(val_precisions) + 1)\n",
    "    metrics_to_plot = [val_precisions, val_recalls, val_f1_scores, val_maps]\n",
    "    metric_labels = ['Precision (%)', 'Recall (%)', 'F1 Score (%)', 'mAP (%)']\n",
    "    metric_colors = ['orange', 'green', 'red', 'purple']\n",
    "    # Function to filter out None values and multiply by 100\n",
    "    def prepare_for_plotting(metrics):\n",
    "        return [m * 100 for m in metrics if m is not None]\n",
    "    # Determine the number of rows needed for subplots\n",
    "    num_metrics = len(metrics_to_plot)\n",
    "    num_rows = (num_metrics + 1) // 2\n",
    "    plt.figure(figsize=(20, 4 * num_rows))\n",
    "    for i, (metrics, label, color) in enumerate(zip(metrics_to_plot, metric_labels, metric_colors), start=1):\n",
    "        plt.subplot(num_rows, 2, i)\n",
    "        plt.plot(valid_epochs, prepare_for_plotting(metrics), color=color, marker='o', linestyle='-', label=f'Validation {label}')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel(label)\n",
    "        plt.title(f'Validation {label} Over Epochs')\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA GPU is available.\n",
      "Device 0: NVIDIA GeForce RTX 4090\n",
      "Epoch 1 of 5\n",
      "Epoch 1/5 - Batch 1/9, Loss: 4.8999\n",
      "Epoch 1/5 - Batch 2/9, Loss: 2.4255\n",
      "Epoch 1/5 - Batch 3/9, Loss: 1.4330\n",
      "Epoch 1/5 - Batch 4/9, Loss: 1.4100\n",
      "Epoch 1/5 - Batch 5/9, Loss: 1.4694\n",
      "Epoch 1/5 - Batch 6/9, Loss: 1.4992\n",
      "Epoch 1/5 - Batch 7/9, Loss: 1.2057\n",
      "Epoch 1/5 - Batch 8/9, Loss: 1.1525\n",
      "Epoch 1/5 - Batch 9/9, Loss: 0.9216\n",
      "Average training loss: 1.8241\n",
      "Training Loss: 1.8241\n",
      "Epoch 2 of 5\n",
      "Epoch 2/5 - Batch 1/9, Loss: 1.0765\n",
      "Epoch 2/5 - Batch 2/9, Loss: 1.2099\n",
      "Epoch 2/5 - Batch 3/9, Loss: 1.1731\n",
      "Epoch 2/5 - Batch 4/9, Loss: 0.9973\n",
      "Epoch 2/5 - Batch 5/9, Loss: 1.4165\n",
      "Epoch 2/5 - Batch 6/9, Loss: 1.2491\n",
      "Epoch 2/5 - Batch 7/9, Loss: 1.2658\n",
      "Epoch 2/5 - Batch 8/9, Loss: 1.2537\n",
      "Epoch 2/5 - Batch 9/9, Loss: 1.2364\n",
      "Average training loss: 1.2087\n",
      "Training Loss: 1.2087\n",
      "Epoch 3 of 5\n",
      "Epoch 3/5 - Batch 1/9, Loss: 1.0831\n",
      "Epoch 3/5 - Batch 2/9, Loss: 1.3121\n",
      "Epoch 3/5 - Batch 3/9, Loss: 1.1852\n",
      "Epoch 3/5 - Batch 4/9, Loss: 1.1105\n",
      "Epoch 3/5 - Batch 5/9, Loss: 1.3077\n",
      "Epoch 3/5 - Batch 6/9, Loss: 1.1715\n",
      "Epoch 3/5 - Batch 7/9, Loss: 1.3235\n",
      "Epoch 3/5 - Batch 8/9, Loss: 1.1557\n",
      "Epoch 3/5 - Batch 9/9, Loss: 1.3385\n",
      "Average training loss: 1.2209\n",
      "Starting validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Metrics:  40%|███▉      | 10540/26522 [03:42<05:43, 46.53it/s]"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "'''\n",
    "it will retrive a pre trained model FASTER-RCNN with RESNET 50 as a backbone and feature pyramid network \n",
    "'''\n",
    "def get_model(num_classes):\n",
    "    model_weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "    model = detection.fasterrcnn_resnet50_fpn(weights=model_weights)\n",
    "    # Replace the classifier with a new one for the number of classes there are\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "# a background class to represent areas that have no acne which is done by incrementing the number of classes by 1\n",
    "num_classes = len(category_id_mapping) + 1\n",
    "model = get_model(num_classes)\n",
    "# CUDA setup and model initialization and check if they are available \n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA GPU is available.\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA GPU is not available.\")\n",
    "#move the model to a specific device \n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# initalize the optimizer with a specific learning rate based on the number of epoches\n",
    "lr = 0.005\n",
    "momentum = 0.9\n",
    "weight_decay = 0.0005\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "# setting up the learning rate scheduler in order to adjust the learning rate based on every epoch \n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "# Initialize Dataset and DataLoader with a specific batch size and number of workers for data loading\n",
    "batchSize = 32\n",
    "numWorkers = 16\n",
    "# sampling a a fixed subset of the dataset for every epoch that we have \n",
    "subset_size = len(train_dataset) // 22\n",
    "subset_sampler = SubsetRandomSampler(range(subset_size))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batchSize, sampler=subset_sampler, collate_fn=collate_fn, num_workers=numWorkers)\n",
    "valid_dataset = AcneSet(root_dir='dataset', set_type='valid', transform=AcneSet.get_transform(train=False))\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batchSize, sampler=subset_sampler, collate_fn=collate_fn, num_workers=numWorkers)\n",
    "# Initialize metric lists\n",
    "train_losses = []\n",
    "val_precisions = []\n",
    "val_recalls = []\n",
    "val_f1_scores = []\n",
    "val_maps = []\n",
    "#Training and Validating Phases\n",
    "num_epochs = 5\n",
    "# iterate over each epoch \n",
    "for epoch in range(num_epochs):\n",
    "    # calling train_epoch whcih is a predefined function to train the model \n",
    "    train_loss = train_epoch(model, train_loader, optimizer, device, epoch, num_epochs)\n",
    "    train_losses.append(train_loss)\n",
    "    # validating over every 3 epoches \n",
    "    if (epoch + 1) % 3 == 0:  \n",
    "        precision, recall, f1_score, mAP = validate(model, valid_loader, device, category_id_mapping)\n",
    "        val_precisions.append(precision)\n",
    "        val_recalls.append(recall)\n",
    "        val_f1_scores.append(f1_score)\n",
    "        val_maps.append(mAP)\n",
    "    else:\n",
    "        # appending the training loss and the validations to their list\n",
    "        val_precisions.append(val_precisions[-1] if val_precisions else None)\n",
    "        val_recalls.append(val_recalls[-1] if val_recalls else None)\n",
    "        val_f1_scores.append(val_f1_scores[-1] if val_f1_scores else None)\n",
    "        val_maps.append(val_maps[-1] if val_maps else None)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Loss = {train_loss:.4f}\")\n",
    "# using a predefined function plot_metrics we are plotting our outcome \n",
    "plot_metrics(train_losses, val_precisions, val_recalls, val_f1_scores, val_maps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
